# Word2Vec

## Введение
Недавно увлекся темой эмбеддингов, и решил подробно изучить каждый из методов обучения эмбеддингов слов с их модификациями.  

## Модели
- CBOW
- Skip-Gram
- FastText
- GloVe

## Модификации класссический методов
У первой тройки реализовал такие методы, как:
1. Negative Sampling
2. Hierarhical Softmax: реализовано на сбалансированном бинарном дереве, может быть сделаю еще релиз на дереве хаффмана (адаптивном)

## Цель

- Реализация всех методов в одном проекте для полного покружения в мир эмбеддингов
- Тест и изучение каждого из методов с программной, математической, философской точки зрения

## Отличие моего проекта от таких реализаций, как от nltk и тд

- Использование torch, а значит и cuda
- Более читаемый код с точки зрения ООП
- Большее кол-во методов для изучения модели

## Визуализация с помощью plotly
### Метрики
![alt text](present/metrics.png)
### Слова в друмерной плоскости
![alt text](present/image.png)
